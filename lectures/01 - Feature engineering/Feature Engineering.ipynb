{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Engineering\n",
    "<center><img src=\"./images/Engineering.jpg\" alt=\"Engineering\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work.\n",
    "... according to Wikipedia.org.  \n",
    "  \n",
    "Typially an iterative process where new features are generated and tested, e.g. batch-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "- GIGO - garbage in, garbage out\n",
    "- Machine Learning methods are limited, cannot make gold from lead\n",
    "- Domain knowledge and random or exhaustive feature engineering can unlock patterns hidden from the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Kaggle competiton/tutorial - Titanic survival\n",
    "- Has been compulsory assignment in DAT200\n",
    "- Few complete features, missing data\n",
    "- Some information needs decoding, e.g. titles from names\n",
    "- Combining features smartly wastly increases accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as ms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_data = pd.read_csv('./data/Titanic_Train.csv')\n",
    "ms.matrix(train_data, figsize=[9,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A standard machine learning pipeline\n",
    "<img src=\"./images/Pipeline.png\" alt=\"Typical Pipeline\"/>  \n",
    "Source: Practical Machine Learning with Python, Apress/Springer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The process of feature engineering (traditionally):\n",
    "1. Brainstorming or Testing features;\n",
    "2. Deciding what features to create;\n",
    "3. Creating features;\n",
    "4. Checking how the features work with your model;\n",
    "5. Improving your features if needed;\n",
    "6. Go back to brainstorming/creating more features until the work is done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Creating features\n",
    "- Should expand the feature space, preferrably in a non-linear fashion.\n",
    "- Simple linear combinations typically do not add anything new.\n",
    "- Polynomial features.\n",
    "    - Interactions (products / quotients).\n",
    "- Non-linear functions of single features or feature combinations\n",
    "- Transformations:\n",
    "    - log\n",
    "    - Box-Cox (typically for non-normally distributed data)\n",
    "<img src=\"./images/Box_Cox.png\" alt=\"Box-Cox transformation\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "--------------- End of first lecture -----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terms from Machine Learning\n",
    "- Feature combinations/crossing\n",
    "  - Combinations that cannot be represented by the linear system, e.g. ReLU and friends.\n",
    "- Feature bucketing\n",
    "  - Create major categories from continuous or multi class data.\n",
    "  - https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization.html -> Colab\n",
    "- Feature templates\n",
    "  - Implicit generation of new features in a model, or\n",
    "  - A group of features all computed in a similar way.\n",
    "      - Length greater than ...\n",
    "      - Last three characters equals ...\n",
    "      - Contains character ...\n",
    "- Feature hashing\n",
    "  - Use hashing algorithms to create vectors/matrices from complicated predictors.\n",
    "  - F.ex. dictionary type terms where the vocabulary may grow.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### scikit-learn's pre-processing\n",
    "- Non-linear transformation by quantiles\n",
    "- Binarization\n",
    "    - 0 or not (1)\n",
    "- OneHot encoding\n",
    "    - Multiple 0 or 1\n",
    "\n",
    "http://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature representation\n",
    "- Re-code timestamps to:\n",
    "    - day of the year\n",
    "    - time of day\n",
    "    - minutes since some event\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/timeseries.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "two_dates = pd.to_datetime(['2018-07-29', '2018-07-30'])\n",
    "print(two_dates)\n",
    "print(two_dates.values.astype('datetime64[m]') + 1)\n",
    "print(two_dates.values+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature representation\n",
    "- Switch between numeric and categorical/ordinal\n",
    "    - Length of education => Achieved degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# EXERCISE: Recode list of integers into list of degrees\n",
    "Assume the following (only integers):\n",
    "    - <3 years => Course shopper\n",
    "    -  3 years => Bachelor\n",
    "    -  5 years => Master\n",
    "    -  8 years => Ph.D.\n",
    "    - 13 years => Overtime\n",
    "\"\"\"\n",
    "eduLength = [3,5,1,3,6,7,4,2,6,8,9,4,2,6,9]\n",
    "degrees = {'Course shopper':1, 'Bachelor':3, 'Master':5, 'Ph.D.':8, 'Overtime':13}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### External data\n",
    "- Time series:\n",
    "    - Connect events on time points\n",
    "- External APIs: \n",
    "    - E.g. Microsoft Computer Vision to count faces in an image (free Azure subscription avilable)\n",
    "- Geocoding:\n",
    "    - Convert between street addresses, coordinates, etc.\n",
    "    - Connect to external data sources based on coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rectified Linear Units - ReLU\n",
    "$f(x)=x^+=max(0,x)$  \n",
    "  \n",
    "Non-linear activation function / transformed feature.  \n",
    "Simple derivative except a discontinuity in 0 $\\Rightarrow$ good candidate for activation function in deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def ReLU(x):\n",
    "    x[x<0] = 0\n",
    "\n",
    "# Plot effect on straight line\n",
    "y = np.arange(-1,1,0.1)\n",
    "fig = plt.figure(figsize=[7,7])\n",
    "plt.plot(y, label='Original')\n",
    "ReLU(y)\n",
    "plt.plot(y, '--', label='ReLU')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variants\n",
    "Soft truncation of $x<0$. In deep neural networks this can help against \"dead neurons\" where a neuron is perpetually inactive. $a$ below is a hyperparameter.  \n",
    "  \n",
    "Leaky ReLU:  \n",
    "$f(x)=max(ax,x)$, for $0 < a < 1$.  \n",
    "  \n",
    "Exponential linear unit (ELU):  \n",
    "$f(x)=max(a(e^x-1),x)$, for $0 < a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def LeakyReLU(x, a):\n",
    "    x[x<0] = a*x[x<0]\n",
    "\n",
    "def ELU(x, a):\n",
    "    x[x<0] = a*(np.exp(x[x<0])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot effect on straight line\n",
    "y = np.arange(-1,1,0.1)\n",
    "fig = plt.figure(figsize=[7,7])\n",
    "plt.plot(y, label='Original')\n",
    "LeakyReLU(y,0.1)\n",
    "plt.plot(y, '--', label='Leaky ReLU')\n",
    "y = np.arange(-1,1,0.1)\n",
    "ELU(y,0.1)\n",
    "plt.plot(y, ':', label='ELU')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "Random normal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = np.sort(np.random.normal(size=[500,1]))\n",
    "yReLU  = y.copy()\n",
    "yLReLU = y.copy()\n",
    "yELU   = y.copy()\n",
    "\n",
    "ReLU(yReLU)\n",
    "LeakyReLU(yLReLU,0.25)\n",
    "ELU(yELU,0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Plot sorted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[7,7])\n",
    "plt.plot(sorted(y), label='Original')\n",
    "plt.plot(sorted(yReLU), label = 'ReLU')\n",
    "plt.plot(sorted(yLReLU), label = 'Leaky ReLU (0.25)')\n",
    "plt.plot(sorted(yELU), label = 'ELU (0.25)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Sorted values')\n",
    "plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ReLU on differences\n",
    "ReLU can also be calcualted on feature combinations, e.g. differences between features, either in a systematic or random way:  \n",
    "  \n",
    "$f(x_i,x_j)=(x_i-x_j)^+=max(0,x_i-x_j)$  \n",
    "  \n",
    "Added to the original values, these can greatly increase the available feature space for regression or classification methods.\n",
    "  \n",
    "![Homer the chef](./images/Bilde1.gif \"Homer the chef\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example with MNIST handwritten digits\n",
    "Perform Logistic Regression with original (scaled) data and data augmented with all pair-wise ReLU'ed differences between pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     12,
     19
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "# Pixels as an array\n",
    "data = digits.data\n",
    "# Classification target (digits)\n",
    "target = digits['target']\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "       train_test_split(data, target, \n",
    "                        test_size=0.3,\n",
    "                        random_state=1)\n",
    "LR = LogisticRegression(penalty='l2', random_state=1)\n",
    "\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "train_scores, test_scores = validation_curve(\n",
    "                estimator=LR, \n",
    "                X=x_train, \n",
    "                y=y_train, \n",
    "                param_name='C', # The paramter to vary\n",
    "                param_range=param_range, # ... and its values\n",
    "                cv=5) # Stratified KFold by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     7,
     10,
     14
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate validation curves for training and test sets\n",
    "train_mean = np.mean(train_scores, axis=1); train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1);   test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean, \n",
    "         color='blue', marker='o', \n",
    "         markersize=5, label='training accuracy')\n",
    "plt.fill_between(param_range, train_mean + train_std,\n",
    "                 train_mean - train_std, alpha=0.15,\n",
    "                 color='blue')\n",
    "plt.plot(param_range, test_mean, \n",
    "         color='green', linestyle='--', \n",
    "         marker='s', markersize=5, \n",
    "         label='validation accuracy')\n",
    "plt.fill_between(param_range, \n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std, \n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.title('Raw pixels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Augmented with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "\n",
    "# Systematic differences between all columns of X\n",
    "def all_differences(X):\n",
    "    n, n_pred = X.shape\n",
    "    j = 0\n",
    "    n_left = n_pred-1\n",
    "    B = np.zeros([n, comb(n_pred,2,True)]);\n",
    "    for i in range(n_pred):\n",
    "        B[:,j+np.arange(n_left)] = X[:,i+1:] - X[:,i,None] # Avoid collapse with None\n",
    "        j += n_left;\n",
    "        n_left -= 1;\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "diff_data = all_differences(data)\n",
    "ReLU(diff_data)\n",
    "data_augmented = np.hstack([data, diff_data])\n",
    "print(data.shape)\n",
    "print(data_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(np.linalg.matrix_rank(data))\n",
    "print(np.linalg.matrix_rank(data_augmented))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "x_train_aug, x_test_aug, y_train, y_test = \\\n",
    "       train_test_split(data_augmented, target, \n",
    "                        test_size=0.3,\n",
    "                        random_state=1)\n",
    "LR = LogisticRegression(penalty='l2', random_state=1)\n",
    "\n",
    "# Cross-validate various L2 parameter values \n",
    "param_range = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "train_scores_aug, test_scores_aug = validation_curve(\n",
    "                estimator=LR, \n",
    "                X=x_train_aug, \n",
    "                y=y_train, \n",
    "                param_name='C', # The paramter to vary\n",
    "                param_range=param_range, # ... and its values\n",
    "                cv=5) # Stratified KFold by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     7,
     10,
     14
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate validation curves for training and test sets\n",
    "train_mean_aug = np.mean(train_scores_aug, axis=1); train_std_aug  = np.std(train_scores_aug, axis=1)\n",
    "test_mean_aug  = np.mean(test_scores_aug, axis=1);  test_std_aug   = np.std(test_scores_aug, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean_aug, \n",
    "         color='blue', marker='o', \n",
    "         markersize=5, label='training accuracy')\n",
    "plt.fill_between(param_range, train_mean_aug + train_std_aug,\n",
    "                 train_mean_aug - train_std_aug, alpha=0.15,\n",
    "                 color='blue')\n",
    "plt.plot(param_range, test_mean_aug, \n",
    "         color='green', linestyle='--', \n",
    "         marker='s', markersize=5, \n",
    "         label='validation accuracy')\n",
    "plt.fill_between(param_range, \n",
    "                 test_mean_aug + test_std_aug,\n",
    "                 test_mean_aug - test_std_aug, \n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.title('Augmented with ReLU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('Maximum accuracy:')\n",
    "print('Original data: {0:.3f}'.format(max(test_mean)))\n",
    "print('Augmented data: {0:.3f}'.format(max(test_mean_aug)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Double augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "diff_data = np.hstack([all_differences(data),-all_differences(data)])\n",
    "ReLU(diff_data)\n",
    "data_augmented2 = np.hstack([data, diff_data])\n",
    "print(data.shape)\n",
    "print(data_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "x_train_aug2, x_test_aug2, y_train, y_test = \\\n",
    "       train_test_split(data_augmented2, target, \n",
    "                        test_size=0.3,\n",
    "                        random_state=1)\n",
    "LR = LogisticRegression(penalty='l2', random_state=1)\n",
    "\n",
    "# Cross-validate various L2 parameter values (almost 2 minutes on teacher's computer)\n",
    "param_range = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "train_scores_aug2, test_scores_aug2 = validation_curve(\n",
    "                estimator=LR, \n",
    "                X=x_train_aug2, \n",
    "                y=y_train, \n",
    "                param_name='C', # The paramter to vary\n",
    "                param_range=param_range, # ... and its values\n",
    "                cv=5) # Stratified KFold by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     7,
     10,
     14
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate validation curves for training and test sets\n",
    "train_mean_aug2 = np.mean(train_scores_aug2, axis=1); train_std_aug2  = np.std(train_scores_aug2, axis=1)\n",
    "test_mean_aug2  = np.mean(test_scores_aug2, axis=1);  test_std_aug2   = np.std(test_scores_aug2, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean_aug2, \n",
    "         color='blue', marker='o', \n",
    "         markersize=5, label='training accuracy')\n",
    "plt.fill_between(param_range, train_mean_aug2 + train_std_aug2,\n",
    "                 train_mean_aug2 - train_std_aug2, alpha=0.15,\n",
    "                 color='blue')\n",
    "plt.plot(param_range, test_mean_aug2, \n",
    "         color='green', linestyle='--', \n",
    "         marker='s', markersize=5, \n",
    "         label='validation accuracy')\n",
    "plt.fill_between(param_range, \n",
    "                 test_mean_aug2 + test_std_aug2,\n",
    "                 test_mean_aug2 - test_std_aug2, \n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.title('Augmented twice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('Maximum cross-validation accuracy:')\n",
    "print('Original data: {0:.3f}'.format(max(test_mean)))\n",
    "print('Augmented data: {0:.3f}'.format(max(test_mean_aug)))\n",
    "print('Double augmented data: {0:.3f}'.format(max(test_mean_aug2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Test set classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "LR.C = param_range[np.argmax(test_mean)]\n",
    "LR.fit(x_train, y_train)\n",
    "accuracy = LR.score(x_test,y_test)\n",
    "\n",
    "# Augmented\n",
    "LR.C = param_range[np.argmax(test_mean_aug)]\n",
    "LR.fit(x_train_aug, y_train)\n",
    "accuracy_aug = LR.score(x_test_aug,y_test)\n",
    "\n",
    "# Double augmented\n",
    "LR.C = param_range[np.argmax(test_mean_aug2)]\n",
    "LR.fit(x_train_aug2, y_train)\n",
    "accuracy_aug2 = LR.score(x_test_aug2,y_test)\n",
    "\n",
    "print('Maximum test set accuracy')\n",
    "print('Original data: {0:.3f}'.format(accuracy))\n",
    "print('Augmented data: {0:.3f}'.format(accuracy_aug))\n",
    "print('Double augmented data: {0:.3f}'.format(accuracy_aug2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Custom bucketing\n",
    "df = pd.DataFrame({'normal': np.random.normal(10, 3, 1000)})\n",
    "print(df.describe().T); print()\n",
    "\n",
    "custom_bucket_array = np.linspace(0, 20, 9)\n",
    "custom_bucket_array = [0, 10, 15, 20]\n",
    "df['bucket'] = pd.cut(df['normal'], custom_bucket_array)\n",
    "print(custom_bucket_array);print()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatized feature engineering:\n",
    "- Deep Feature Synthesis (MIT)\n",
    "- OneBM (IBM)\n",
    "- ExploreKit (Berkeley)\n",
    "<img src=\"./images/DFS.png\" alt=\"Deep feature syntehsis\" style=\"width: 300px;\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Feature Synthesis is available in Python through Featuretools:\n",
    "- automatic feature engineering for relational and timestamped data\n",
    "  - e.g. database exports with common keys, time aspect, ...\n",
    "- mimics human made feature combinations and transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Feature Synthesis:\n",
    "- aggregate data from one table and merge into another (one-to-many relation),  \n",
    "  e.g. sum, std., max, min., mean, count, percent true, num unique, mode, trend, skew, custom (agg_primitives)\n",
    "- transformations in the primary data table (trans_primitives)\n",
    "- combinations of the above\n",
    "- aggregation of aggregations (depth = 2, deep feature), and deeper (harder to interpret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example using single DataFrame\n",
    "\n",
    "- Iris data\n",
    "    - Three species, four attributes\n",
    "- No relational database or time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import featuretools as ft\n",
    "\n",
    "# Load data and put into dataframe\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "df['species'] = iris.target\n",
    "df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make an entityset and add the entity\n",
    "es = ft.EntitySet(id = 'iris')\n",
    "es.entity_from_dataframe(entity_id = 'data', dataframe = df, \n",
    "                         make_index = True, index = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# List of available primitives\n",
    "ft.list_primitives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Run deep feature synthesis with transformation primitives\n",
    "feature_matrix, feature_defs = ft.dfs(entityset = es, target_entity = 'data',\n",
    "                                      trans_primitives = ['add_numeric', 'multiply_numeric'])#, max_depth=2)\n",
    "feature_matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(iris.data, df['species'].values)\n",
    "clf.score(iris.data, df['species'].values)\n",
    "cross_val_score(clf, iris.data, df['species'].values, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(feature_matrix.drop('species',axis=1).values, df['species'].values)\n",
    "clf.score(feature_matrix.drop('species',axis=1).values, df['species'].values)\n",
    "cross_val_score(clf, feature_matrix.drop('species',axis=1).values, df['species'].values, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "#### Prediction of olympic medals\n",
    "Browse the following:  \n",
    "https://github.com/Featuretools/predict-olympic-medals/blob/master/PredictOlympicMedals.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Box-Cox example\n",
    "<img src=\"./images/Box_Cox.png\" alt=\"Box-Cox transformation\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "non_normal = np.random.gamma(1, size=[600])\n",
    "bc, param = ss.boxcox(non_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(non_normal)\n",
    "plt.title('Gamma distr.')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(bc)\n",
    "plt.title('Box-Cox transformed')\n",
    "plt.show()\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Encoder example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "fishNchips = np.array(['fish','chips','fish','fish','chips','mayonnaise'])\n",
    "print(fishNchips); print(' ')\n",
    "\n",
    "fcIntegers = LabelEncoder().fit_transform(fishNchips) # Redundant for OHE in newer scikit-learn\n",
    "print(fcIntegers); print(' ')\n",
    "\n",
    "fcOneHot   = OneHotEncoder().fit_transform(fcIntegers[:,np.newaxis]).toarray()\n",
    "print(fcOneHot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "someData = np.array([[1,2],[1,5],[0,2],[1,0],[3,1]])\n",
    "print(someData); print(' ')\n",
    "print(PolynomialFeatures(2).fit_transform(someData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error analysis:\n",
    "- Inspect some large errors\n",
    "- Inspect class-wise\n",
    "- Run clustering on errors to search for common patterns\n",
    "- Ask a colleague or expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "endofcell": "--",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "- Feature engineering has various definitions and supposed workflows/content\n",
    "- Apply (non-linear) functions on original data\n",
    "    - Direct transformations\n",
    "    - Feature combinations\n",
    "- Aim: Aid Machine Learning methods, unlock hidden patterns\n",
    "- Deep feature synthesis\n",
    "    - Automatic FE\n",
    "    - Mainly for relational data(bases)\n",
    "- Exercises in GitLab:\n",
    "    - Flubby Flobs (detective work for recreating features)\n",
    "    - Titanic survival (download from Kaggle to Colab, basic handling)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "145px",
    "left": "1376.45px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
