{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Engineering\n",
    "<img src=\"./images/Engineering.jpg\" alt=\"Engineering\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work.\n",
    "... according to Wikipedia.org.  \n",
    "  \n",
    "Typially an iterative process where new features are generated and tested, e.g. batch-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "- GIGO - garbage in, garbage out\n",
    "- Machine Learning methods are limited, cannot make gold from lead\n",
    "- Domain knowledge and random or exhaustive feature engineering can unlock patterns hidden from the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Kaggle competiton/tutorial - Titanic survival\n",
    "- 3rd compulsory assignment in DAT200\n",
    "- Few complete features, missing data\n",
    "- Some information needs decoding, e.g. titles from names\n",
    "- Combining features smartly wastly increases accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as ms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_data = pd.read_csv('./data/Titanic_Train.csv')\n",
    "ms.matrix(train_data, figsize=[9,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A standard machine learning pipeline\n",
    "<img src=\"./images/Pipeline.png\" alt=\"Typical Pipeline\"/>  \n",
    "Source: Practical Machine Learning with Python, Apress/Springer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The process of feature engineering (traditionally):\n",
    "1. Brainstorming or Testing features;\n",
    "2. Deciding what features to create;\n",
    "3. Creating features;\n",
    "4. Checking how the features work with your model;\n",
    "5. Improving your features if needed;\n",
    "6. Go back to brainstorming/creating more features until the work is done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Creating features\n",
    "- Should expand the feature space, preferrably in a non-linear fashion.\n",
    "- Simple linear combinations typically do not add anything new.\n",
    "- Polynomial features.\n",
    "    - Interactions (products / quotients).\n",
    "- Non-linear functions of single features or feature combinations\n",
    "- Transformations:\n",
    "    - log, Box-Cox\n",
    "<img src=\"./images/Box_Cox.png\" alt=\"Box-Cox transformation\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terms from Machine Learning\n",
    "- Feature combinations/crossing\n",
    "  - Combinations that cannot be represented by the linear system, e.g. ReLU and friends.\n",
    "- Feature bucketing\n",
    "  - Create major categories from continuous or multi class data.\n",
    "- Feature templates\n",
    "  - Implicit generation of new features in a model, or\n",
    "  - A group of features all computed in a similar way.\n",
    "      - Length greater than ...\n",
    "      - Last three characters equals ...\n",
    "      - Contains character ...\n",
    "- Feature hashing\n",
    "  - Use hashing algorithms to create vectors/matrices from complicated predictors.\n",
    "  - F.ex. dictionary type terms where the vocabulary may grow.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### scikit-learn's pre-processing\n",
    "- Non-linear transformation by quantiles\n",
    "- Binarization\n",
    "    - 0 or not (1)\n",
    "- OneHot encoding\n",
    "    - Multiple 0 or 1\n",
    "\n",
    "http://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature representation\n",
    "- Re-code timestamps to:\n",
    "    - day of the year\n",
    "    - time of day\n",
    "    - minutes since some event\n",
    "    - https://pandas.pydata.org/pandas-docs/stable/timeseries.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "two_dates = pd.to_datetime(['2018-07-29', '2018-07-30'])\n",
    "print(two_dates)\n",
    "print(two_dates.values.astype('datetime64[m]') + 1)\n",
    "print(two_dates.values+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature representation\n",
    "- Switch between numeric and categorical/ordinal\n",
    "    - Length of education => Achieved degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### External data\n",
    "- Time series:\n",
    "    - Connect events on time points\n",
    "- External APIs: \n",
    "    - E.g. Microsoft Computer Vision to count faces in an image (free Azure subscription avilable)\n",
    "- Geocoding:\n",
    "    - Convert between street addresses, coordinates, etc.\n",
    "    - Connect to external data sources based on coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rectified Linear Units - ReLU\n",
    "$f(x)=x^+=max(0,x)$  \n",
    "  \n",
    "Non-linear activation function / transformed feature.  \n",
    "Simple derivative except a discontinuity in 0 $\\Rightarrow$ good candidate for activation function in deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def ReLU(x):\n",
    "    x[x<0] = 0\n",
    "\n",
    "# Plot effect on straight line\n",
    "y = np.arange(-1,1,0.1)\n",
    "fig = plt.figure(figsize=[7,7])\n",
    "plt.plot(y, label='Original')\n",
    "ReLU(y)\n",
    "plt.plot(y, '--', label='ReLU')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variants\n",
    "Soft truncation of $x<0$. In deep neural networks this can help against \"dead neurons\" where a neuron is perpetually inactive. $a$ below is a hyperparameter.  \n",
    "  \n",
    "Leaky ReLU:  \n",
    "$f(x)=max(ax,x)$, for $0 < a < 1$.  \n",
    "  \n",
    "Exponential linear unit (ELU):  \n",
    "$f(x)=max(a(e^x-1),x)$, for $0 < a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def LeakyReLU(x, a):\n",
    "    x[x<0] = a*x[x<0]\n",
    "\n",
    "def ELU(x, a):\n",
    "    x[x<0] = a*(np.exp(x[x<0])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot effect on straight line\n",
    "y = np.arange(-1,1,0.1)\n",
    "fig = plt.figure(figsize=[7,7])\n",
    "plt.plot(y, label='Original')\n",
    "LeakyReLU(y,0.1)\n",
    "plt.plot(y, '--', label='Leaky ReLU')\n",
    "y = np.arange(-1,1,0.1)\n",
    "ELU(y,0.1)\n",
    "plt.plot(y, ':', label='ELU')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "Random normal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = np.sort(np.random.normal(size=[500,1]))\n",
    "yReLU  = y.copy()\n",
    "yLReLU = y.copy()\n",
    "yELU   = y.copy()\n",
    "\n",
    "ReLU(yReLU)\n",
    "LeakyReLU(yLReLU,0.25)\n",
    "ELU(yELU,0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Plot sorted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[7,7])\n",
    "plt.plot(sorted(y), label='Original')\n",
    "plt.plot(sorted(yReLU), label = 'ReLU')\n",
    "plt.plot(sorted(yLReLU), label = 'Leaky ReLU (0.25)')\n",
    "plt.plot(sorted(yELU), label = 'ELU (0.25)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Sorted values')\n",
    "plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ReLU on differences\n",
    "ReLU can also be calcualted on feature combinations, e.g. differences between features, either in a systematic or random way:  \n",
    "  \n",
    "$f(x_i,x_j)=(x_i-x_j)^+=max(0,x_i-x_j)$  \n",
    "  \n",
    "Added to the original values, these can greatly increase the available feature space for regression or classification methods.\n",
    "  \n",
    "![Homer the chef](./images/Bilde1.gif \"Homer the chef\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example with MNIST handwritten digits\n",
    "Perform Logistic Regression with original (scaled) data and data augmented with all pair-wise ReLU'ed differences between pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     12,
     19
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "# Pixels as an array\n",
    "data = digits.data\n",
    "# Classification target (digits)\n",
    "target = digits['target']\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "       train_test_split(data, target, \n",
    "                        test_size=0.3,\n",
    "                        random_state=1)\n",
    "LR = LogisticRegression(penalty='l2', random_state=1)\n",
    "\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "train_scores, test_scores = validation_curve(\n",
    "                estimator=LR, \n",
    "                X=x_train, \n",
    "                y=y_train, \n",
    "                param_name='C', # The paramter to vary\n",
    "                param_range=param_range, # ... and its values\n",
    "                cv=5) # Stratified KFold by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     7,
     10,
     14
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate validation curves for training and test sets\n",
    "train_mean = np.mean(train_scores, axis=1); train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1);   test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean, \n",
    "         color='blue', marker='o', \n",
    "         markersize=5, label='training accuracy')\n",
    "plt.fill_between(param_range, train_mean + train_std,\n",
    "                 train_mean - train_std, alpha=0.15,\n",
    "                 color='blue')\n",
    "plt.plot(param_range, test_mean, \n",
    "         color='green', linestyle='--', \n",
    "         marker='s', markersize=5, \n",
    "         label='validation accuracy')\n",
    "plt.fill_between(param_range, \n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std, \n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.title('Raw pixels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Augmented with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "\n",
    "# Systematic differences between all columns of X\n",
    "def all_differences(X):\n",
    "    n, n_pred = X.shape\n",
    "    j = 0\n",
    "    n_left = n_pred-1\n",
    "    B = np.zeros([n, comb(n_pred,2,True)]);\n",
    "    for i in range(n_pred):\n",
    "        B[:,j+np.arange(n_left)] = X[:,i+1:] - X[:,i,None] # Avoid collapse with None\n",
    "        j += n_left;\n",
    "        n_left -= 1;\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "diff_data = all_differences(data)\n",
    "ReLU(diff_data)\n",
    "data_augmented = np.hstack([data, diff_data])\n",
    "print(data.shape)\n",
    "print(data_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(np.linalg.matrix_rank(data))\n",
    "print(np.linalg.matrix_rank(data_augmented))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "x_train_aug, x_test_aug, y_train, y_test = \\\n",
    "       train_test_split(data_augmented, target, \n",
    "                        test_size=0.3,\n",
    "                        random_state=1)\n",
    "LR = LogisticRegression(penalty='l2', random_state=1)\n",
    "\n",
    "# Cross-validate various L2 parameter values \n",
    "param_range = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "train_scores_aug, test_scores_aug = validation_curve(\n",
    "                estimator=LR, \n",
    "                X=x_train_aug, \n",
    "                y=y_train, \n",
    "                param_name='C', # The paramter to vary\n",
    "                param_range=param_range, # ... and its values\n",
    "                cv=5) # Stratified KFold by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     7,
     10,
     14
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate validation curves for training and test sets\n",
    "train_mean_aug = np.mean(train_scores_aug, axis=1); train_std_aug  = np.std(train_scores_aug, axis=1)\n",
    "test_mean_aug  = np.mean(test_scores_aug, axis=1);  test_std_aug   = np.std(test_scores_aug, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean_aug, \n",
    "         color='blue', marker='o', \n",
    "         markersize=5, label='training accuracy')\n",
    "plt.fill_between(param_range, train_mean_aug + train_std_aug,\n",
    "                 train_mean_aug - train_std_aug, alpha=0.15,\n",
    "                 color='blue')\n",
    "plt.plot(param_range, test_mean_aug, \n",
    "         color='green', linestyle='--', \n",
    "         marker='s', markersize=5, \n",
    "         label='validation accuracy')\n",
    "plt.fill_between(param_range, \n",
    "                 test_mean_aug + test_std_aug,\n",
    "                 test_mean_aug - test_std_aug, \n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.title('Augmented with ReLU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('Maximum accuracy:')\n",
    "print('Original data: {0:.3f}'.format(max(test_mean)))\n",
    "print('Augmented data: {0:.3f}'.format(max(test_mean_aug)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Double augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "diff_data = np.hstack([all_differences(data),-all_differences(data)])\n",
    "ReLU(diff_data)\n",
    "data_augmented2 = np.hstack([data, diff_data])\n",
    "print(data.shape)\n",
    "print(data_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "x_train_aug2, x_test_aug2, y_train, y_test = \\\n",
    "       train_test_split(data_augmented2, target, \n",
    "                        test_size=0.3,\n",
    "                        random_state=1)\n",
    "LR = LogisticRegression(penalty='l2', random_state=1)\n",
    "\n",
    "# Cross-validate various L2 parameter values (almost 2 minutes on teacher's computer)\n",
    "param_range = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "train_scores_aug2, test_scores_aug2 = validation_curve(\n",
    "                estimator=LR, \n",
    "                X=x_train_aug2, \n",
    "                y=y_train, \n",
    "                param_name='C', # The paramter to vary\n",
    "                param_range=param_range, # ... and its values\n",
    "                cv=5) # Stratified KFold by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     7,
     10,
     14
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate validation curves for training and test sets\n",
    "train_mean_aug2 = np.mean(train_scores_aug2, axis=1); train_std_aug2  = np.std(train_scores_aug2, axis=1)\n",
    "test_mean_aug2  = np.mean(test_scores_aug2, axis=1);  test_std_aug2   = np.std(test_scores_aug2, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean_aug2, \n",
    "         color='blue', marker='o', \n",
    "         markersize=5, label='training accuracy')\n",
    "plt.fill_between(param_range, train_mean_aug2 + train_std_aug2,\n",
    "                 train_mean_aug2 - train_std_aug2, alpha=0.15,\n",
    "                 color='blue')\n",
    "plt.plot(param_range, test_mean_aug2, \n",
    "         color='green', linestyle='--', \n",
    "         marker='s', markersize=5, \n",
    "         label='validation accuracy')\n",
    "plt.fill_between(param_range, \n",
    "                 test_mean_aug2 + test_std_aug2,\n",
    "                 test_mean_aug2 - test_std_aug2, \n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.title('Augmented twice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('Maximum cross-validation accuracy:')\n",
    "print('Original data: {0:.3f}'.format(max(test_mean)))\n",
    "print('Augmented data: {0:.3f}'.format(max(test_mean_aug)))\n",
    "print('Double augmented data: {0:.3f}'.format(max(test_mean_aug2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Test set classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "LR.C = param_range[np.argmax(test_mean)]\n",
    "LR.fit(x_train, y_train)\n",
    "accuracy = LR.score(x_test,y_test)\n",
    "\n",
    "# Augmented\n",
    "LR.C = param_range[np.argmax(test_mean_aug)]\n",
    "LR.fit(x_train_aug, y_train)\n",
    "accuracy_aug = LR.score(x_test_aug,y_test)\n",
    "\n",
    "# Double augmented\n",
    "LR.C = param_range[np.argmax(test_mean_aug2)]\n",
    "LR.fit(x_train_aug2, y_train)\n",
    "accuracy_aug2 = LR.score(x_test_aug2,y_test)\n",
    "\n",
    "print('Maximum test set accuracy')\n",
    "print('Original data: {0:.3f}'.format(accuracy))\n",
    "print('Augmented data: {0:.3f}'.format(accuracy_aug))\n",
    "print('Double augmented data: {0:.3f}'.format(accuracy_aug2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Custom bucketing\n",
    "df = pd.DataFrame({'normal': np.random.normal(10, 3, 1000)})\n",
    "print(df.describe().T); print()\n",
    "\n",
    "custom_bucket_array = np.linspace(0, 20, 9)\n",
    "custom_bucket_array = [0, 10, 15, 20]\n",
    "df['bucket'] = pd.cut(df['normal'], custom_bucket_array)\n",
    "print(custom_bucket_array);print()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatized feature engineering:\n",
    "- Deep Feature Synthesis (MIT)\n",
    "- OneBM (IBM)\n",
    "- ExploreKit (Berkeley)\n",
    "<img src=\"./images/DFS.png\" alt=\"Deep feature syntehsis\" style=\"width: 300px;\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Feature Synthesis is available in Python through Featuretools:\n",
    "- automatic feature engineering for relational and timestamped data\n",
    "  - e.g. database exports with common keys, time aspect, ...\n",
    "- mimics human made feature combinations and transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deep Feature Synthesis:\n",
    "- aggregate data from one table and merge into another (one-to-many relation),  \n",
    "  e.g. sum, std., max, min., mean, count, percent true, num unique, mode, trend, skew, custom\n",
    "- transformations in the primary data table\n",
    "- combinations of the above\n",
    "- aggregation of aggregations (depth = 2, deep feature), and deeper (harder to interpret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# -------------------------------------\n",
    "# TODO: Update from here!\n",
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "#### Prediction of olympic medals\n",
    "Copied from:  \n",
    "https://github.com/Featuretools/predict-olympic-medals/blob/master/PredictOlympicMedals.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Imputer, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import featuretools as ft\n",
    "import utils as utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "label_df = pd.read_csv('./data/num_medals_by_country_labels.csv',\n",
    "                       parse_dates=['Olympics Date'],\n",
    "                       encoding='utf-8',\n",
    "                       usecols=['Number of Medals', 'Olympics Date', 'Country'])\n",
    "label_df.sort_values(['Olympics Date', 'Country'], inplace=True)\n",
    "es = utils.load_entityset(\"./data/\") # This line does important preprocessing in a different script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make a new label for binary classification\n",
    "dates = label_df['Olympics Date']\n",
    "labels = label_df['Number of Medals']\n",
    "y_binary = (labels >= 10).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Introduce cut-off time\n",
    "DFS will not let the feature generation process be influenced by data from the same year/country as is being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cutoff_times = label_df[['Country', 'Olympics Date']].rename(columns={'Country': 'Code'})\n",
    "cutoff_times.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Deep feature synthesis\n",
    "- Extract data from external datasets (dictionary, summer and winter)\n",
    "- Aggregate data primitives (up to 3 levels of aggregation here) and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "agg_primitives = ['Sum', 'Std', 'Max', 'Min', 'Mean', \n",
    "                  'Count', 'Percent_True', 'Num_Unique', \n",
    "                  'Mode', 'Trend', 'Skew']\n",
    "\n",
    "feature_matrix, features = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_entity=\"countries\",\n",
    "    trans_primitives=[],\n",
    "    agg_primitives=agg_primitives,\n",
    "    max_depth=3,\n",
    "    cutoff_time=cutoff_times,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"{} features generated\".format(len(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Manipulate data\n",
    "- Impute missing\n",
    "- Scale before modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "feature_matrix_encoded, features_encoded = ft.encode_features(feature_matrix, features)\n",
    "\n",
    "pipeline_preprocessing = [(\"imputer\",\n",
    "                           Imputer(missing_values='NaN', strategy=\"mean\", axis=0)),\n",
    "                          (\"scaler\", RobustScaler(with_centering=True))]\n",
    "feature_matrix_encoded.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Perform Random Forests classification\n",
    "- Combine pre-processing and classifier.\n",
    "- Predict >=10 or <10 medals for each included country for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "splitter = utils.TimeSeriesSplitByDate(dates=dates, earliest_date=pd.Timestamp('1/1/1960'))\n",
    "X = feature_matrix_encoded.values\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n",
    "pipeline = Pipeline(pipeline_preprocessing + [('rf_clf', rf_clf)])\n",
    "binary_scores = utils.fit_and_score(X, y_binary, splitter, pipeline, _type='classification')\n",
    "\"Average AUC score is {} with standard dev {}\".format(\n",
    "        round(binary_scores['roc_auc'].mean(), 3),\n",
    "        round(np.std(binary_scores['roc_auc']), 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Result analysis\n",
    "AUC improved from 0.79 without DFS to 0.95 with DFS.  \n",
    "  \n",
    "Performance per year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "binary_scores.set_index('Olympics Year')['roc_auc'].plot(title='AUC vs. Olympics Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 1984 = 5th split, a bad year prediction-wise\n",
    "split, year = 5, '1984'\n",
    "train, test = splitter.split(X, y_binary)[split]\n",
    "pipeline.fit(X[train], y_binary[train])\n",
    "y_pred = pipeline.predict(X[test])\n",
    "cm = confusion_matrix(y_binary[test], y_pred)\n",
    "utils.plot_confusion_matrix(cm, ['Won < 10 Medals', 'Won >= 10 Medals'], title=year)\n",
    "\n",
    "# Possibly explainable by the Soviet countries' block of the olympics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 2004 = 10th split, a good year prediction-wise\n",
    "split, year = 10, '2004'\n",
    "train, test = splitter.split(X, y_binary)[split]\n",
    "pipeline.fit(X[train], y_binary[train])\n",
    "y_pred = pipeline.predict(X[test])\n",
    "cm = confusion_matrix(y_binary[test], y_pred)\n",
    "utils.plot_confusion_matrix(cm, ['Won < 10 Medals', 'Won >= 10 Medals'], title=year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Feature importance\n",
    "We can inspect the feature importance at any timepoint, e.g. 1984:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get feature importances for every year\n",
    "feature_imp = utils.get_feature_importances(pipeline, \n",
    "                                            feature_matrix_encoded, \n",
    "                                            (labels >= 10), splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Show 10 most important features for 1984\n",
    "test_date = pd.Timestamp('6/29/1984')\n",
    "display(feature_imp[test_date].iloc[:5].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show 10 most important features for 2004\n",
    "test_date = pd.Timestamp('6/29/2004')\n",
    "display(feature_imp[test_date].iloc[:5].reset_index(drop=True))\n",
    "print(feature_imp[test_date].iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Box-Cox example\n",
    "<img src=\"./images/Box_Cox.png\" alt=\"Box-Cox transformation\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "non_normal = np.random.gamma(1, size=[600])\n",
    "bc, param = ss.boxcox(non_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(non_normal)\n",
    "plt.title('Gamma distr.')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(bc)\n",
    "plt.title('Box-Cox transformed')\n",
    "plt.show()\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Encoder example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "fishNchips = np.array(['fish','chips','fish','fish','chips','mayonnaise'])\n",
    "print(fishNchips); print(' ')\n",
    "\n",
    "fcIntegers = LabelEncoder().fit_transform(fishNchips)\n",
    "print(fcIntegers); print(' ')\n",
    "\n",
    "fcOneHot   = OneHotEncoder().fit_transform(fcIntegers[:,np.newaxis]).toarray()\n",
    "print(fcOneHot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "someData = np.array([[1,2],[1,5],[0,2],[1,0],[3,1]])\n",
    "print(someData); print(' ')\n",
    "print(PolynomialFeatures(2).fit_transform(someData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error analysis:\n",
    "- Inspect some large errors\n",
    "- Inspect class-wise\n",
    "- Run clustering on errors to search for common patterns\n",
    "- Ask a colleague or expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "- \n",
    "- "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "145px",
    "left": "1376.45px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
