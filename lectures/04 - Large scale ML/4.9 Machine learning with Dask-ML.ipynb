{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.9 Machine learning with Dask-ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Subjects covered*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Building machine learning models using the Dask-ML API\n",
    "* Using the Dask-ML API to extend scikit-learn\n",
    "* Validating models and tuning hyperparameters using cross-validated gridsearch\n",
    "* Using serialization to save and publish trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Content*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Text data preparation using Dask bags](#Text-data-preparation-using-Dask-bags)\n",
    "- [Building linear models with Dask-ML](#Building-linear-models-with-Dask-ML)\n",
    "- [Evaluating and tuning Dask-ML models](#Evaluating-and-tuning-Dask-ML-models)\n",
    "- [Persisting Dask-ML models](#Persisting-Dask-ML-models)\n",
    "- [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data preparation using Dask bags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tagging the review data based on the review score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data here: https://snap.stanford.edu/data/web-FineFoods.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.1\n",
    "import dask.bag as bag\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# private laptop\n",
    "directory_path = 'C:\\\\Users\\\\olive\\\\Desktop\\\\DAT300\\\\'\n",
    "\n",
    "# work laptop\n",
    "#directory_path = 'C:\\\\Users\\\\olto\\\\Desktop\\\\DAT300\\\\'\n",
    "\n",
    "file_name = 'foods.txt'\n",
    "file_path = directory_path + file_name\n",
    "raw_data = bag.read_text(file_path)\n",
    "\n",
    "def get_next_part(file, start_index, span_index=0, blocksize=1024):\n",
    "    file.seek(start_index)\n",
    "    buffer = file.read(blocksize + span_index).decode('cp1252')\n",
    "    delimiter_position = buffer.find('\\n\\n')\n",
    "    if delimiter_position == -1:\n",
    "        return get_next_part(file, start_index, span_index + blocksize)\n",
    "    else:\n",
    "        file.seek(start_index)\n",
    "        return start_index, delimiter_position\n",
    "    \n",
    "def get_item(filename, start_index, delimiter_position, encoding='cp1252'):\n",
    "    with open(filename, 'rb') as file_handle:\n",
    "        file_handle.seek(start_index)\n",
    "        text = file_handle.read(delimiter_position).decode(encoding)\n",
    "        elements = text.strip().split('\\n')\n",
    "        key_value_pairs = [(element.split(': ')[0], element.split(': ')[1]) \n",
    "                               if len(element.split(': ')) > 1 \n",
    "                               else ('unknown', element) \n",
    "                               for element in elements]\n",
    "        return dict(key_value_pairs)\n",
    "    \n",
    "with open(file_path, 'rb') as file_handle:\n",
    "    size = file_handle.seek(0,2) - 1\n",
    "    more_data = True\n",
    "    output = []\n",
    "    current_position = next_position = 0\n",
    "    while more_data:\n",
    "        if current_position >= size:\n",
    "            more_data = False\n",
    "        else:\n",
    "            current_position, next_position = get_next_part(file_handle, current_position, 0)\n",
    "            output.append((current_position, next_position))\n",
    "            current_position = current_position + next_position + 2\n",
    "            \n",
    "reviews = bag.from_sequence(output).map(lambda x: get_item(file_path, x[0], x[1]))\n",
    "\n",
    "def tag_positive_negative_by_score(element):\n",
    "    if float(element['review/score']) > 3:\n",
    "        element['review/sentiment'] = 'positive'\n",
    "    else:\n",
    "        element['review/sentiment'] = 'negative'\n",
    "    return element\n",
    "\n",
    "tagged_reviews = reviews.map(tag_positive_negative_by_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_reviews.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing text and Removing stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.2\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from functools import partial\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def extract_reviews(element):\n",
    "    element['review/tokens'] = element['review/text'].lower()\n",
    "    return element\n",
    "\n",
    "def tokenize_reviews(element):\n",
    "    element['review/tokens'] = tokenizer.tokenize(element['review/tokens'])\n",
    "    return element\n",
    "\n",
    "def filter_stopword(word, stopwords):\n",
    "    return word not in stopwords\n",
    "\n",
    "def filter_stopwords(element, stopwords):\n",
    "    element['review/tokens'] = list(filter(partial(filter_stopword, stopwords=stopwords), element['review/tokens']))\n",
    "    return element\n",
    "\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "more_stopwords = {'br', 'amazon', 'com', 'http', 'www', 'href', 'gp'}\n",
    "all_stopwords = stopword_set.union(more_stopwords)\n",
    "\n",
    "review_extracted_text = tagged_reviews.map(extract_reviews)\n",
    "review_tokens = review_extracted_text.map(tokenize_reviews)\n",
    "review_text_clean = review_tokens.map(partial(filter_stopwords, stopwords=all_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counting the unique words in the Amazon Fine Foods review set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.3\n",
    "def extract_tokens(element):\n",
    "    return element['review/tokens']\n",
    "\n",
    "extracted_tokens = review_text_clean.map(extract_tokens)\n",
    "unique_tokens = extracted_tokens.flatten().distinct()\n",
    "\n",
    "with ProgressBar():\n",
    "    number_of_tokens = unique_tokens.count().compute()\n",
    "number_of_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the top 100 most common words in the reviews dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.4\n",
    "def count(accumulator, element):\n",
    "    return accumulator + 1\n",
    "\n",
    "def combine(total_1, total_2):\n",
    "    return total_1 + total_2\n",
    "\n",
    "with ProgressBar():\n",
    "    token_counts = extracted_tokens.flatten().foldby(lambda x: x, count, 0, combine, 0).compute()\n",
    "    \n",
    "top_tokens = sorted(token_counts, key=lambda x: x[1], reverse=True)\n",
    "top_100_tokens = list(map(lambda x: x[0], top_tokens[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating training data by applying binary vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.5\n",
    "import numpy as np\n",
    "def vectorize_tokens(element):\n",
    "    vectorized_tokens = np.where(np.isin(top_100_tokens, element['review/tokens']), 1, 0)\n",
    "    element['review/token_vector'] = vectorized_tokens\n",
    "    return element\n",
    "\n",
    "def prep_model_data(element):\n",
    "    return {'target': 1 if element['review/sentiment'] == 'positive' else 0,\n",
    "            'features': element['review/token_vector']}\n",
    "\n",
    "model_data = review_text_clean.map(vectorize_tokens).map(prep_model_data)\n",
    "\n",
    "model_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the feature array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.6\n",
    "from dask import array as dask_array\n",
    "def stacker(partition):\n",
    "    return dask_array.concatenate([element for element in partition])\n",
    "\n",
    "with ProgressBar():\n",
    "    feature_arrays = model_data.pluck('features').map(lambda x: dask_array.from_array(x, 1000).reshape(1,-1)).reduction(perpartition=stacker, aggregate=stacker)\n",
    "    feature_array = feature_arrays.compute()\n",
    "feature_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing the data to ZARR and reading it back in**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "You need to install the **zarr** Python package to be able to run this code, since the data are stored in the ZARR format. It is available in the Anaconda distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.7\n",
    "with ProgressBar():\n",
    "    feature_array.rechunk(5000).to_zarr('sentiment_feature_array.zarr')\n",
    "    feature_array = dask_array.from_zarr('sentiment_feature_array.zarr')\n",
    "    \n",
    "with ProgressBar():\n",
    "    target_arrays = model_data.pluck('target').map(lambda x: dask_array.from_array(x, 1000).reshape(-1,1)).reduction(perpartition=stacker, aggregate=stacker)\n",
    "    target_arrays.compute().rechunk(5000).to_zarr('sentiment_target_array.zarr')\n",
    "    target_array = dask_array.from_zarr('sentiment_target_array.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building linear models with Dask-ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 10.8\n",
    "from dask_ml.linear_model import LogisticRegression\n",
    "from dask_ml.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_array\n",
    "y = target_array.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>  <thead>    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 227.38 MB </td> <td> 2.00 MB </td></tr>\n",
       "    <tr><th> Shape </th><td> (568454, 100) </td> <td> (5000, 100) </td></tr>\n",
       "    <tr><th> Count </th><td> 115 Tasks </td><td> 114 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> int32 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody></table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"1\" x2=\"25\" y2=\"1\" />\n",
       "  <line x1=\"0\" y1=\"2\" x2=\"25\" y2=\"2\" />\n",
       "  <line x1=\"0\" y1=\"3\" x2=\"25\" y2=\"3\" />\n",
       "  <line x1=\"0\" y1=\"4\" x2=\"25\" y2=\"4\" />\n",
       "  <line x1=\"0\" y1=\"5\" x2=\"25\" y2=\"5\" />\n",
       "  <line x1=\"0\" y1=\"6\" x2=\"25\" y2=\"6\" />\n",
       "  <line x1=\"0\" y1=\"7\" x2=\"25\" y2=\"7\" />\n",
       "  <line x1=\"0\" y1=\"8\" x2=\"25\" y2=\"8\" />\n",
       "  <line x1=\"0\" y1=\"9\" x2=\"25\" y2=\"9\" />\n",
       "  <line x1=\"0\" y1=\"10\" x2=\"25\" y2=\"10\" />\n",
       "  <line x1=\"0\" y1=\"11\" x2=\"25\" y2=\"11\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"25\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"13\" x2=\"25\" y2=\"13\" />\n",
       "  <line x1=\"0\" y1=\"14\" x2=\"25\" y2=\"14\" />\n",
       "  <line x1=\"0\" y1=\"15\" x2=\"25\" y2=\"15\" />\n",
       "  <line x1=\"0\" y1=\"16\" x2=\"25\" y2=\"16\" />\n",
       "  <line x1=\"0\" y1=\"17\" x2=\"25\" y2=\"17\" />\n",
       "  <line x1=\"0\" y1=\"18\" x2=\"25\" y2=\"18\" />\n",
       "  <line x1=\"0\" y1=\"20\" x2=\"25\" y2=\"20\" />\n",
       "  <line x1=\"0\" y1=\"21\" x2=\"25\" y2=\"21\" />\n",
       "  <line x1=\"0\" y1=\"22\" x2=\"25\" y2=\"22\" />\n",
       "  <line x1=\"0\" y1=\"23\" x2=\"25\" y2=\"23\" />\n",
       "  <line x1=\"0\" y1=\"24\" x2=\"25\" y2=\"24\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"0\" y1=\"26\" x2=\"25\" y2=\"26\" />\n",
       "  <line x1=\"0\" y1=\"27\" x2=\"25\" y2=\"27\" />\n",
       "  <line x1=\"0\" y1=\"28\" x2=\"25\" y2=\"28\" />\n",
       "  <line x1=\"0\" y1=\"29\" x2=\"25\" y2=\"29\" />\n",
       "  <line x1=\"0\" y1=\"30\" x2=\"25\" y2=\"30\" />\n",
       "  <line x1=\"0\" y1=\"31\" x2=\"25\" y2=\"31\" />\n",
       "  <line x1=\"0\" y1=\"32\" x2=\"25\" y2=\"32\" />\n",
       "  <line x1=\"0\" y1=\"33\" x2=\"25\" y2=\"33\" />\n",
       "  <line x1=\"0\" y1=\"34\" x2=\"25\" y2=\"34\" />\n",
       "  <line x1=\"0\" y1=\"35\" x2=\"25\" y2=\"35\" />\n",
       "  <line x1=\"0\" y1=\"36\" x2=\"25\" y2=\"36\" />\n",
       "  <line x1=\"0\" y1=\"37\" x2=\"25\" y2=\"37\" />\n",
       "  <line x1=\"0\" y1=\"39\" x2=\"25\" y2=\"39\" />\n",
       "  <line x1=\"0\" y1=\"40\" x2=\"25\" y2=\"40\" />\n",
       "  <line x1=\"0\" y1=\"41\" x2=\"25\" y2=\"41\" />\n",
       "  <line x1=\"0\" y1=\"42\" x2=\"25\" y2=\"42\" />\n",
       "  <line x1=\"0\" y1=\"43\" x2=\"25\" y2=\"43\" />\n",
       "  <line x1=\"0\" y1=\"44\" x2=\"25\" y2=\"44\" />\n",
       "  <line x1=\"0\" y1=\"45\" x2=\"25\" y2=\"45\" />\n",
       "  <line x1=\"0\" y1=\"46\" x2=\"25\" y2=\"46\" />\n",
       "  <line x1=\"0\" y1=\"47\" x2=\"25\" y2=\"47\" />\n",
       "  <line x1=\"0\" y1=\"48\" x2=\"25\" y2=\"48\" />\n",
       "  <line x1=\"0\" y1=\"49\" x2=\"25\" y2=\"49\" />\n",
       "  <line x1=\"0\" y1=\"50\" x2=\"25\" y2=\"50\" />\n",
       "  <line x1=\"0\" y1=\"51\" x2=\"25\" y2=\"51\" />\n",
       "  <line x1=\"0\" y1=\"52\" x2=\"25\" y2=\"52\" />\n",
       "  <line x1=\"0\" y1=\"53\" x2=\"25\" y2=\"53\" />\n",
       "  <line x1=\"0\" y1=\"54\" x2=\"25\" y2=\"54\" />\n",
       "  <line x1=\"0\" y1=\"55\" x2=\"25\" y2=\"55\" />\n",
       "  <line x1=\"0\" y1=\"56\" x2=\"25\" y2=\"56\" />\n",
       "  <line x1=\"0\" y1=\"58\" x2=\"25\" y2=\"58\" />\n",
       "  <line x1=\"0\" y1=\"59\" x2=\"25\" y2=\"59\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"25\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"61\" x2=\"25\" y2=\"61\" />\n",
       "  <line x1=\"0\" y1=\"62\" x2=\"25\" y2=\"62\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"64\" x2=\"25\" y2=\"64\" />\n",
       "  <line x1=\"0\" y1=\"65\" x2=\"25\" y2=\"65\" />\n",
       "  <line x1=\"0\" y1=\"66\" x2=\"25\" y2=\"66\" />\n",
       "  <line x1=\"0\" y1=\"67\" x2=\"25\" y2=\"67\" />\n",
       "  <line x1=\"0\" y1=\"68\" x2=\"25\" y2=\"68\" />\n",
       "  <line x1=\"0\" y1=\"69\" x2=\"25\" y2=\"69\" />\n",
       "  <line x1=\"0\" y1=\"70\" x2=\"25\" y2=\"70\" />\n",
       "  <line x1=\"0\" y1=\"71\" x2=\"25\" y2=\"71\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"25\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"73\" x2=\"25\" y2=\"73\" />\n",
       "  <line x1=\"0\" y1=\"74\" x2=\"25\" y2=\"74\" />\n",
       "  <line x1=\"0\" y1=\"75\" x2=\"25\" y2=\"75\" />\n",
       "  <line x1=\"0\" y1=\"77\" x2=\"25\" y2=\"77\" />\n",
       "  <line x1=\"0\" y1=\"78\" x2=\"25\" y2=\"78\" />\n",
       "  <line x1=\"0\" y1=\"79\" x2=\"25\" y2=\"79\" />\n",
       "  <line x1=\"0\" y1=\"80\" x2=\"25\" y2=\"80\" />\n",
       "  <line x1=\"0\" y1=\"81\" x2=\"25\" y2=\"81\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"83\" x2=\"25\" y2=\"83\" />\n",
       "  <line x1=\"0\" y1=\"84\" x2=\"25\" y2=\"84\" />\n",
       "  <line x1=\"0\" y1=\"85\" x2=\"25\" y2=\"85\" />\n",
       "  <line x1=\"0\" y1=\"86\" x2=\"25\" y2=\"86\" />\n",
       "  <line x1=\"0\" y1=\"87\" x2=\"25\" y2=\"87\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"25\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"89\" x2=\"25\" y2=\"89\" />\n",
       "  <line x1=\"0\" y1=\"90\" x2=\"25\" y2=\"90\" />\n",
       "  <line x1=\"0\" y1=\"91\" x2=\"25\" y2=\"91\" />\n",
       "  <line x1=\"0\" y1=\"92\" x2=\"25\" y2=\"92\" />\n",
       "  <line x1=\"0\" y1=\"93\" x2=\"25\" y2=\"93\" />\n",
       "  <line x1=\"0\" y1=\"94\" x2=\"25\" y2=\"94\" />\n",
       "  <line x1=\"0\" y1=\"96\" x2=\"25\" y2=\"96\" />\n",
       "  <line x1=\"0\" y1=\"97\" x2=\"25\" y2=\"97\" />\n",
       "  <line x1=\"0\" y1=\"98\" x2=\"25\" y2=\"98\" />\n",
       "  <line x1=\"0\" y1=\"99\" x2=\"25\" y2=\"99\" />\n",
       "  <line x1=\"0\" y1=\"100\" x2=\"25\" y2=\"100\" />\n",
       "  <line x1=\"0\" y1=\"101\" x2=\"25\" y2=\"101\" />\n",
       "  <line x1=\"0\" y1=\"102\" x2=\"25\" y2=\"102\" />\n",
       "  <line x1=\"0\" y1=\"103\" x2=\"25\" y2=\"103\" />\n",
       "  <line x1=\"0\" y1=\"104\" x2=\"25\" y2=\"104\" />\n",
       "  <line x1=\"0\" y1=\"105\" x2=\"25\" y2=\"105\" />\n",
       "  <line x1=\"0\" y1=\"106\" x2=\"25\" y2=\"106\" />\n",
       "  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n",
       "  <line x1=\"0\" y1=\"108\" x2=\"25\" y2=\"108\" />\n",
       "  <line x1=\"0\" y1=\"109\" x2=\"25\" y2=\"109\" />\n",
       "  <line x1=\"0\" y1=\"110\" x2=\"25\" y2=\"110\" />\n",
       "  <line x1=\"0\" y1=\"111\" x2=\"25\" y2=\"111\" />\n",
       "  <line x1=\"0\" y1=\"112\" x2=\"25\" y2=\"112\" />\n",
       "  <line x1=\"0\" y1=\"113\" x2=\"25\" y2=\"113\" />\n",
       "  <line x1=\"0\" y1=\"115\" x2=\"25\" y2=\"115\" />\n",
       "  <line x1=\"0\" y1=\"116\" x2=\"25\" y2=\"116\" />\n",
       "  <line x1=\"0\" y1=\"117\" x2=\"25\" y2=\"117\" />\n",
       "  <line x1=\"0\" y1=\"118\" x2=\"25\" y2=\"118\" />\n",
       "  <line x1=\"0\" y1=\"119\" x2=\"25\" y2=\"119\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 25.412617,0.000000 25.412617,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >100</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">568454</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<from-zarr, shape=(568454, 100), dtype=int32, chunksize=(5000, 100)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>  <thead>    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 2.27 MB </td> <td> 20.00 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (568454,) </td> <td> (5000,) </td></tr>\n",
       "    <tr><th> Count </th><td> 229 Tasks </td><td> 114 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> int32 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody></table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"1\" y1=\"0\" x2=\"1\" y2=\"25\" />\n",
       "  <line x1=\"2\" y1=\"0\" x2=\"2\" y2=\"25\" />\n",
       "  <line x1=\"3\" y1=\"0\" x2=\"3\" y2=\"25\" />\n",
       "  <line x1=\"4\" y1=\"0\" x2=\"4\" y2=\"25\" />\n",
       "  <line x1=\"5\" y1=\"0\" x2=\"5\" y2=\"25\" />\n",
       "  <line x1=\"6\" y1=\"0\" x2=\"6\" y2=\"25\" />\n",
       "  <line x1=\"7\" y1=\"0\" x2=\"7\" y2=\"25\" />\n",
       "  <line x1=\"8\" y1=\"0\" x2=\"8\" y2=\"25\" />\n",
       "  <line x1=\"9\" y1=\"0\" x2=\"9\" y2=\"25\" />\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"25\" />\n",
       "  <line x1=\"11\" y1=\"0\" x2=\"11\" y2=\"25\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"25\" />\n",
       "  <line x1=\"13\" y1=\"0\" x2=\"13\" y2=\"25\" />\n",
       "  <line x1=\"14\" y1=\"0\" x2=\"14\" y2=\"25\" />\n",
       "  <line x1=\"15\" y1=\"0\" x2=\"15\" y2=\"25\" />\n",
       "  <line x1=\"16\" y1=\"0\" x2=\"16\" y2=\"25\" />\n",
       "  <line x1=\"17\" y1=\"0\" x2=\"17\" y2=\"25\" />\n",
       "  <line x1=\"18\" y1=\"0\" x2=\"18\" y2=\"25\" />\n",
       "  <line x1=\"20\" y1=\"0\" x2=\"20\" y2=\"25\" />\n",
       "  <line x1=\"21\" y1=\"0\" x2=\"21\" y2=\"25\" />\n",
       "  <line x1=\"22\" y1=\"0\" x2=\"22\" y2=\"25\" />\n",
       "  <line x1=\"23\" y1=\"0\" x2=\"23\" y2=\"25\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"25\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"26\" y1=\"0\" x2=\"26\" y2=\"25\" />\n",
       "  <line x1=\"27\" y1=\"0\" x2=\"27\" y2=\"25\" />\n",
       "  <line x1=\"28\" y1=\"0\" x2=\"28\" y2=\"25\" />\n",
       "  <line x1=\"29\" y1=\"0\" x2=\"29\" y2=\"25\" />\n",
       "  <line x1=\"30\" y1=\"0\" x2=\"30\" y2=\"25\" />\n",
       "  <line x1=\"31\" y1=\"0\" x2=\"31\" y2=\"25\" />\n",
       "  <line x1=\"32\" y1=\"0\" x2=\"32\" y2=\"25\" />\n",
       "  <line x1=\"33\" y1=\"0\" x2=\"33\" y2=\"25\" />\n",
       "  <line x1=\"34\" y1=\"0\" x2=\"34\" y2=\"25\" />\n",
       "  <line x1=\"35\" y1=\"0\" x2=\"35\" y2=\"25\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"25\" />\n",
       "  <line x1=\"37\" y1=\"0\" x2=\"37\" y2=\"25\" />\n",
       "  <line x1=\"39\" y1=\"0\" x2=\"39\" y2=\"25\" />\n",
       "  <line x1=\"40\" y1=\"0\" x2=\"40\" y2=\"25\" />\n",
       "  <line x1=\"41\" y1=\"0\" x2=\"41\" y2=\"25\" />\n",
       "  <line x1=\"42\" y1=\"0\" x2=\"42\" y2=\"25\" />\n",
       "  <line x1=\"43\" y1=\"0\" x2=\"43\" y2=\"25\" />\n",
       "  <line x1=\"44\" y1=\"0\" x2=\"44\" y2=\"25\" />\n",
       "  <line x1=\"45\" y1=\"0\" x2=\"45\" y2=\"25\" />\n",
       "  <line x1=\"46\" y1=\"0\" x2=\"46\" y2=\"25\" />\n",
       "  <line x1=\"47\" y1=\"0\" x2=\"47\" y2=\"25\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"25\" />\n",
       "  <line x1=\"49\" y1=\"0\" x2=\"49\" y2=\"25\" />\n",
       "  <line x1=\"50\" y1=\"0\" x2=\"50\" y2=\"25\" />\n",
       "  <line x1=\"51\" y1=\"0\" x2=\"51\" y2=\"25\" />\n",
       "  <line x1=\"52\" y1=\"0\" x2=\"52\" y2=\"25\" />\n",
       "  <line x1=\"53\" y1=\"0\" x2=\"53\" y2=\"25\" />\n",
       "  <line x1=\"54\" y1=\"0\" x2=\"54\" y2=\"25\" />\n",
       "  <line x1=\"55\" y1=\"0\" x2=\"55\" y2=\"25\" />\n",
       "  <line x1=\"56\" y1=\"0\" x2=\"56\" y2=\"25\" />\n",
       "  <line x1=\"58\" y1=\"0\" x2=\"58\" y2=\"25\" />\n",
       "  <line x1=\"59\" y1=\"0\" x2=\"59\" y2=\"25\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"25\" />\n",
       "  <line x1=\"61\" y1=\"0\" x2=\"61\" y2=\"25\" />\n",
       "  <line x1=\"62\" y1=\"0\" x2=\"62\" y2=\"25\" />\n",
       "  <line x1=\"63\" y1=\"0\" x2=\"63\" y2=\"25\" />\n",
       "  <line x1=\"64\" y1=\"0\" x2=\"64\" y2=\"25\" />\n",
       "  <line x1=\"65\" y1=\"0\" x2=\"65\" y2=\"25\" />\n",
       "  <line x1=\"66\" y1=\"0\" x2=\"66\" y2=\"25\" />\n",
       "  <line x1=\"67\" y1=\"0\" x2=\"67\" y2=\"25\" />\n",
       "  <line x1=\"68\" y1=\"0\" x2=\"68\" y2=\"25\" />\n",
       "  <line x1=\"69\" y1=\"0\" x2=\"69\" y2=\"25\" />\n",
       "  <line x1=\"70\" y1=\"0\" x2=\"70\" y2=\"25\" />\n",
       "  <line x1=\"71\" y1=\"0\" x2=\"71\" y2=\"25\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"25\" />\n",
       "  <line x1=\"73\" y1=\"0\" x2=\"73\" y2=\"25\" />\n",
       "  <line x1=\"74\" y1=\"0\" x2=\"74\" y2=\"25\" />\n",
       "  <line x1=\"75\" y1=\"0\" x2=\"75\" y2=\"25\" />\n",
       "  <line x1=\"77\" y1=\"0\" x2=\"77\" y2=\"25\" />\n",
       "  <line x1=\"78\" y1=\"0\" x2=\"78\" y2=\"25\" />\n",
       "  <line x1=\"79\" y1=\"0\" x2=\"79\" y2=\"25\" />\n",
       "  <line x1=\"80\" y1=\"0\" x2=\"80\" y2=\"25\" />\n",
       "  <line x1=\"81\" y1=\"0\" x2=\"81\" y2=\"25\" />\n",
       "  <line x1=\"82\" y1=\"0\" x2=\"82\" y2=\"25\" />\n",
       "  <line x1=\"83\" y1=\"0\" x2=\"83\" y2=\"25\" />\n",
       "  <line x1=\"84\" y1=\"0\" x2=\"84\" y2=\"25\" />\n",
       "  <line x1=\"85\" y1=\"0\" x2=\"85\" y2=\"25\" />\n",
       "  <line x1=\"86\" y1=\"0\" x2=\"86\" y2=\"25\" />\n",
       "  <line x1=\"87\" y1=\"0\" x2=\"87\" y2=\"25\" />\n",
       "  <line x1=\"88\" y1=\"0\" x2=\"88\" y2=\"25\" />\n",
       "  <line x1=\"89\" y1=\"0\" x2=\"89\" y2=\"25\" />\n",
       "  <line x1=\"90\" y1=\"0\" x2=\"90\" y2=\"25\" />\n",
       "  <line x1=\"91\" y1=\"0\" x2=\"91\" y2=\"25\" />\n",
       "  <line x1=\"92\" y1=\"0\" x2=\"92\" y2=\"25\" />\n",
       "  <line x1=\"93\" y1=\"0\" x2=\"93\" y2=\"25\" />\n",
       "  <line x1=\"94\" y1=\"0\" x2=\"94\" y2=\"25\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"25\" />\n",
       "  <line x1=\"97\" y1=\"0\" x2=\"97\" y2=\"25\" />\n",
       "  <line x1=\"98\" y1=\"0\" x2=\"98\" y2=\"25\" />\n",
       "  <line x1=\"99\" y1=\"0\" x2=\"99\" y2=\"25\" />\n",
       "  <line x1=\"100\" y1=\"0\" x2=\"100\" y2=\"25\" />\n",
       "  <line x1=\"101\" y1=\"0\" x2=\"101\" y2=\"25\" />\n",
       "  <line x1=\"102\" y1=\"0\" x2=\"102\" y2=\"25\" />\n",
       "  <line x1=\"103\" y1=\"0\" x2=\"103\" y2=\"25\" />\n",
       "  <line x1=\"104\" y1=\"0\" x2=\"104\" y2=\"25\" />\n",
       "  <line x1=\"105\" y1=\"0\" x2=\"105\" y2=\"25\" />\n",
       "  <line x1=\"106\" y1=\"0\" x2=\"106\" y2=\"25\" />\n",
       "  <line x1=\"107\" y1=\"0\" x2=\"107\" y2=\"25\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"25\" />\n",
       "  <line x1=\"109\" y1=\"0\" x2=\"109\" y2=\"25\" />\n",
       "  <line x1=\"110\" y1=\"0\" x2=\"110\" y2=\"25\" />\n",
       "  <line x1=\"111\" y1=\"0\" x2=\"111\" y2=\"25\" />\n",
       "  <line x1=\"112\" y1=\"0\" x2=\"112\" y2=\"25\" />\n",
       "  <line x1=\"113\" y1=\"0\" x2=\"113\" y2=\"25\" />\n",
       "  <line x1=\"115\" y1=\"0\" x2=\"115\" y2=\"25\" />\n",
       "  <line x1=\"116\" y1=\"0\" x2=\"116\" y2=\"25\" />\n",
       "  <line x1=\"117\" y1=\"0\" x2=\"117\" y2=\"25\" />\n",
       "  <line x1=\"118\" y1=\"0\" x2=\"118\" y2=\"25\" />\n",
       "  <line x1=\"119\" y1=\"0\" x2=\"119\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 120.000000,0.000000 120.000000,25.412617 0.000000,25.412617\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >568454</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<reshape, shape=(568454,), dtype=int32, chunksize=(5000,)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ``train_test_split`` function by default does a 90/10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``fit`` method is not lazy (``.compute()`` is not needed), meaning computations will start right away after calling ``fit``. Therefore, to monitor progress ``ProgressBar`` is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2.1s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  6.2s\n",
      "[########################################] | 100% Completed |  7.7s\n",
      "[########################################] | 100% Completed |  5.9s\n",
      "[########################################] | 100% Completed |  5.8s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  6.2s\n",
      "[########################################] | 100% Completed |  7.0s\n",
      "[########################################] | 100% Completed |  6.6s\n",
      "[########################################] | 100% Completed |  5.8s\n",
      "[########################################] | 100% Completed |  6.8s\n",
      "[########################################] | 100% Completed |  6.5s\n",
      "[########################################] | 100% Completed |  6.2s\n",
      "[########################################] | 100% Completed |  6.3s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  5.8s\n",
      "[########################################] | 100% Completed |  6.6s\n",
      "[########################################] | 100% Completed |  5.8s\n",
      "[########################################] | 100% Completed |  5.6s\n",
      "[########################################] | 100% Completed |  6.2s\n",
      "[########################################] | 100% Completed |  5.6s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  5.8s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  6.6s\n",
      "[########################################] | 100% Completed |  6.3s\n",
      "[########################################] | 100% Completed |  6.7s\n",
      "[########################################] | 100% Completed |  6.2s\n",
      "[########################################] | 100% Completed |  5.9s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  5.9s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  6.2s\n",
      "[########################################] | 100% Completed |  6.0s\n",
      "[########################################] | 100% Completed |  5.9s\n",
      "[########################################] | 100% Completed |  6.3s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  5.8s\n",
      "[########################################] | 100% Completed |  5.9s\n",
      "[########################################] | 100% Completed |  6.2s\n",
      "[########################################] | 100% Completed |  6.2s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  6.7s\n",
      "[########################################] | 100% Completed |  5.9s\n",
      "[########################################] | 100% Completed |  6.0s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  5.8s\n",
      "[########################################] | 100% Completed |  6.2s\n",
      "[########################################] | 100% Completed |  6.2s\n",
      "[########################################] | 100% Completed |  6.4s\n",
      "[########################################] | 100% Completed |  6.0s\n",
      "[########################################] | 100% Completed |  5.6s\n",
      "[########################################] | 100% Completed |  6.0s\n",
      "[########################################] | 100% Completed |  6.0s\n",
      "[########################################] | 100% Completed |  5.8s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  6.7s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  6.0s\n",
      "[########################################] | 100% Completed |  6.4s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  6.0s\n",
      "[########################################] | 100% Completed |  6.7s\n",
      "[########################################] | 100% Completed |  5.9s\n",
      "[########################################] | 100% Completed |  6.4s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  5.9s\n",
      "[########################################] | 100% Completed |  5.9s\n",
      "[########################################] | 100% Completed |  6.4s\n",
      "[########################################] | 100% Completed |  5.8s\n",
      "[########################################] | 100% Completed |  6.3s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  5.8s\n",
      "[########################################] | 100% Completed |  6.0s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  6.0s\n",
      "[########################################] | 100% Completed |  6.4s\n",
      "[########################################] | 100% Completed |  6.3s\n",
      "[########################################] | 100% Completed |  6.7s\n",
      "[########################################] | 100% Completed |  6.5s\n",
      "[########################################] | 100% Completed |  6.4s\n",
      "[########################################] | 100% Completed |  5.7s\n",
      "[########################################] | 100% Completed |  5.9s\n",
      "[########################################] | 100% Completed |  6.0s\n",
      "[########################################] | 100% Completed |  5.4s\n",
      "[########################################] | 100% Completed |  6.1s\n",
      "[########################################] | 100% Completed |  6.0s\n",
      "[########################################] | 100% Completed |  5.5s\n",
      "[########################################] | 100% Completed |  6.2s\n",
      "[########################################] | 100% Completed |  5.6s\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating and tuning Dask-ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scoring the logistic regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7968370685712275"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 10.9\n",
    "lr.score(X_test, y_test).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn and ``partial_fit``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``partial_fit`` method is available on some models to allow for batch training. This\n",
    "allows you to effectively “update” a model with additional data rather than retraining\n",
    "from scratch every time your training data has been refreshed. It can also be used to\n",
    "train models from large datasets that can’t be held in memory all at once. For example,\n",
    "a model could be trained by loading 1,000 rows of a DataFrame, training the model on\n",
    "those 1,000 rows, loading the next 1,000 rows, continuing to train, and so forth. Dask‑ML\n",
    "uses this interface to train scikit-learn models with minimal configuration by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models in scikit-learn that implement ``partial_fit`` (Incremental learning)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/computing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a naïve Bayes classifier with the Incremental wrapper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import a naive Bayes classifier from scikit-learn\n",
    "* Wrap the estimator in the ``Incremental`` wrapper\n",
    "* Call ``fit`` on the ``Incremental`` wrapped estimator\n",
    "* Note that the classes **must be predefined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2.4s\n"
     ]
    }
   ],
   "source": [
    "# Listing 10.10\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from dask_ml.wrappers import Incremental\n",
    "\n",
    "nb = BernoulliNB()\n",
    "\n",
    "parallel_nb = Incremental(nb)\n",
    "\n",
    "with ProgressBar():\n",
    "    parallel_nb.fit(X_train, y_train, classes=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A growing number of scikit-learn algorithms are supporting this interface because there has been a growing interest in batch learning for large datasets\n",
    "* The naïve Bayes algorithms fall into the group of algorithms that support batch learning, so they can easily be used with Dask to parallelize training\n",
    "* The Incremental wrapper is essentially a helperfunction that tells Dask about the estimator object so it can pass it off to the workers for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scoring the Incremental wrapped model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7898356964430215"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 10.11\n",
    "parallel_nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using ``GridSearchCV`` to tune hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 28min 34.8s\n",
      "[########################################] | 100% Completed |  6min 19.7s\n"
     ]
    }
   ],
   "source": [
    "# Listing 10.12\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'penalty': ['l1', 'l2'], 'C': [0.5, 1, 2]}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "tuned_lr = GridSearchCV(lr, parameters)\n",
    "\n",
    "with ProgressBar():\n",
    "    tuned_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ``GridSearchCV`` object behaves like the ``Incremental`` wrapper\n",
    "    * Each model can be built on a separate worker\n",
    "    * Search time can be reduced much by deploying a cluster or scaling up number of workers\n",
    "* Take any algorithm, such as Dask-ML’s logistic regression, and wrap it in ``GridSearchCV``\n",
    "* Note: ``GridSearchCV`` may give you memory errors\n",
    "* Alternative: ``IncrementalSearchCV`` (should be used when your full dataset doesn’t fit in memory on a single machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualising the results of ``GridSearchCV``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'C': 0.5, 'penalty': 'l1'}</td>\n",
       "      <td>855.061492</td>\n",
       "      <td>0.486273</td>\n",
       "      <td>0.497073</td>\n",
       "      <td>0.072905</td>\n",
       "      <td>0.790660</td>\n",
       "      <td>0.794090</td>\n",
       "      <td>0.796506</td>\n",
       "      <td>0.793752</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 0.5, 'penalty': 'l2'}</td>\n",
       "      <td>106.952981</td>\n",
       "      <td>0.184854</td>\n",
       "      <td>0.598966</td>\n",
       "      <td>0.036569</td>\n",
       "      <td>0.790836</td>\n",
       "      <td>0.793821</td>\n",
       "      <td>0.796477</td>\n",
       "      <td>0.793711</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
       "      <td>789.225405</td>\n",
       "      <td>46.388903</td>\n",
       "      <td>0.551647</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>0.790736</td>\n",
       "      <td>0.793668</td>\n",
       "      <td>0.796483</td>\n",
       "      <td>0.793629</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
       "      <td>62.377400</td>\n",
       "      <td>0.207629</td>\n",
       "      <td>0.556441</td>\n",
       "      <td>0.033168</td>\n",
       "      <td>0.790836</td>\n",
       "      <td>0.793821</td>\n",
       "      <td>0.796477</td>\n",
       "      <td>0.793711</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'C': 2, 'penalty': 'l1'}</td>\n",
       "      <td>346.042469</td>\n",
       "      <td>75.587293</td>\n",
       "      <td>0.401259</td>\n",
       "      <td>0.150740</td>\n",
       "      <td>0.790707</td>\n",
       "      <td>0.793727</td>\n",
       "      <td>0.796512</td>\n",
       "      <td>0.793649</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'C': 2, 'penalty': 'l2'}</td>\n",
       "      <td>50.758152</td>\n",
       "      <td>1.961505</td>\n",
       "      <td>0.405135</td>\n",
       "      <td>0.086069</td>\n",
       "      <td>0.790836</td>\n",
       "      <td>0.793821</td>\n",
       "      <td>0.796477</td>\n",
       "      <td>0.793711</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        params  mean_fit_time  std_fit_time  mean_score_time  \\\n",
       "0  {'C': 0.5, 'penalty': 'l1'}     855.061492      0.486273         0.497073   \n",
       "1  {'C': 0.5, 'penalty': 'l2'}     106.952981      0.184854         0.598966   \n",
       "2    {'C': 1, 'penalty': 'l1'}     789.225405     46.388903         0.551647   \n",
       "3    {'C': 1, 'penalty': 'l2'}      62.377400      0.207629         0.556441   \n",
       "4    {'C': 2, 'penalty': 'l1'}     346.042469     75.587293         0.401259   \n",
       "5    {'C': 2, 'penalty': 'l2'}      50.758152      1.961505         0.405135   \n",
       "\n",
       "   std_score_time  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0        0.072905           0.790660           0.794090           0.796506   \n",
       "1        0.036569           0.790836           0.793821           0.796477   \n",
       "2        0.027690           0.790736           0.793668           0.796483   \n",
       "3        0.033168           0.790836           0.793821           0.796477   \n",
       "4        0.150740           0.790707           0.793727           0.796512   \n",
       "5        0.086069           0.790836           0.793821           0.796477   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score param_C param_penalty  \n",
       "0         0.793752        0.002399                1     0.5            l1  \n",
       "1         0.793711        0.002304                2     0.5            l2  \n",
       "2         0.793629        0.002346                6       1            l1  \n",
       "3         0.793711        0.002304                2       1            l2  \n",
       "4         0.793649        0.002371                5       2            l1  \n",
       "5         0.793711        0.002304                2       2            l2  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 10.13\n",
    "import pandas as pd\n",
    "pd.DataFrame(tuned_lr.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get hyperparameters of best performing model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.5, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print best parameters\n",
    "tuned_lr.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting Dask-ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Persisting a trained model allows for\n",
    "    * quickly accessing model from where it is stored\n",
    "    * publishing and sharing model with others\n",
    "    * deploying model elswhere\n",
    "    * distributed learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Less powerful machines can use the model for predictions\n",
    "* Prediction usually less resource consuming than model traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('naive_bayes_model.pkl', 'wb') as file:\n",
    "    dill.dump(parallel_nb, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model saved using Python built-in library ``pickle``\n",
    "* ``pickle`` is a binary serialisation library\n",
    "* ``dill`` is a wrapper around ``pickle`` library and has better support for complex structures\n",
    "* ``dump`` function serialises the object and writes it to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Because pickle files are binary files, it always needs to be read and write with the ``b`` flag in the file handle to denote that the file should be opened in binary mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('naive_bayes_model.pkl', 'rb') as file:\n",
    "    nb = dill.load(file)\n",
    "nb.predict(np.random.randint(0, 2, (100, 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training models using algorithms implemented in \n",
    "    * Dask-ML\n",
    "    * scikit-learn implementing ``partial_fit`` with Dask-ML, using ``Incremental`` wrapper\n",
    "* Trained machine learning models can be saved using the ``dill`` library to reuse later to generate predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
