# -*- coding: utf-8 -*-
"""IMDB_TRYING_VW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EXuGw7BljreGxOH3AQOdXJeKBOqNnYEo
"""

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/My Drive/Colab Notebooks"

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/movie_data.csv')
df.head()

import re
def preprocessor(text):
    # Regular expression for HTML tags
    text = re.sub('<[^>]*>', '', text)
    
    # Most typical emoticons (smileys)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)',
                           text)
    
    # Remove all non-word characters, convert to lower-case and add possible emoticons to the end.
    text = (re.sub('[\W]+', ' ', text.lower()) +
            ' '.join(emoticons).replace('-', ''))
    return text

# This takes a few seconds
df['review'] = df['review'].apply(preprocessor)

df['review'].values

import nltk

# Update to most resent stop-words
nltk.download('stopwords')

from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()

# Define basic tokenizer and Porter stemmer version
def tokenizer(text):
    return text.split()

def tokenizer_porter(text):
    return [porter.stem(word) for word in text.split()]

from nltk.corpus import stopwords

# Combine tokenizer with Porter stemmer and stop-word removal
stop = stopwords.words('english')
#[w for w in tokenizer_porter(df['review'].values)
#if w not in stop]

#[w for w in tokenizer_porter('a runner likes running and runs a lot')
#if w not in stop]

X_train = df.loc[:25000, 'review'].values
y_train = df.loc[:25000, 'sentiment'].values
X_test = df.loc[25000:, 'review'].values
y_test = df.loc[25000:, 'sentiment'].values

stops = []
for s in stop:
    stops.append(tokenizer(s)[0])
stopsPorter = []
for s in stop:
    stopsPorter.append(tokenizer_porter(s)[0])

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import GridSearchCV

# TfidfVectorizer combines CountVectorizer and TfidTransformer with a single function.
tfidf = TfidfVectorizer(strip_accents=None, # Already preprocessed
                        lowercase=False,
                        preprocessor=None)

param_grid = [{'vect__ngram_range': [(1, 1)],
               'vect__stop_words': [stops, None], # Not this time, but use idf with normalization
               'vect__tokenizer': [tokenizer],
               'clf__penalty': ['l1', 'l2'],
               'clf__C': [1.0, 10.0, 100.0]},
              {'vect__ngram_range': [(1, 1)],
               'vect__stop_words': [stops, None], # Not this time
               'vect__tokenizer': [tokenizer],
               'vect__use_idf':[False],       # Raw counts without normalization 
               'vect__norm':[None],           # --------------||----------------
               'clf__penalty': ['l1', 'l2'],
               'clf__C': [1.0, 10.0, 100.0]},
             {'vect__ngram_range': [(1, 1)],
               'vect__stop_words': [stopsPorter, None], # Not this time, but use idf with normalization
               'vect__tokenizer': [tokenizer_porter],
               'clf__penalty': ['l1', 'l2'],
               'clf__C': [1.0, 10.0, 100.0]},
              {'vect__ngram_range': [(1, 1)],
               'vect__stop_words': [stopsPorter, None], # Not this time
               'vect__tokenizer': [tokenizer_porter],
               'vect__use_idf':[False],       # Raw counts without normalization 
               'vect__norm':[None],           # --------------||----------------
               'clf__penalty': ['l1', 'l2'],
               'clf__C': [1.0, 10.0, 100.0]}]

lr_tfidf = Pipeline([('vect', tfidf),
                     ('clf', LogisticRegression(random_state=0, solver='saga'))])
# Solver specified to silence warning and to enable l1 regularization

gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,
                           scoring='accuracy',
                           cv=5,
                           verbose=2,
                           n_jobs=1) # Number of jobs different from 1 sometimes crashes on Windows.

# The fitting of 2*2*2*3*5*2 models took around 30-60 minutes to fit in 2018. In 2019 it takes several hours. :(.
# Lowering the number of samples or parameters will make it quicker, but may reduce the performance greatly.
gs_lr_tfidf.fit(X_train, y_train)

print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)
print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)

clf = gs_lr_tfidf.best_estimator_
print('Test Accuracy: %.3f' % clf.score(X_test, y_test))

"""# Load pikcle data and Implement NAIVE gaussian clf"""

from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()

# Define basic tokenizer and Porter stemmer version
def tokenizer(text):
    return text.split()

def tokenizer_porter(text):
    return [porter.stem(word) for word in text.split()]

import nltk

# Update to most resent stop-words
nltk.download('stopwords')

from nltk.corpus import stopwords

# Combine tokenizer with Porter stemmer and stop-word removal
stop = stopwords.words('english')
#[w for w in tokenizer_porter('a runner likes running and runs a lot')
#if w not in stop]

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/movie_data.csv')
df.head()

X_train = df.loc[:25000, 'review'].values
y_train = df.loc[:25000, 'sentiment'].values
X_test = df.loc[25000:, 'review'].values
y_test = df.loc[25000:, 'sentiment'].values

stops = []
for s in stop:
    stops.append(tokenizer(s)[0])
stopsPorter = []
for s in stop:
    stopsPorter.append(tokenizer_porter(s)[0])

# To open an object that has been pickled, you need to import the object's dependencies and local functions
import pickle
with open('/content/drive/My Drive/Colab Notebooks/gs_lr_tfidf.pickle', 'rb') as f:
    gs_lr_tfidf = pickle.load(f)

print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)
print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)

"""#Use of NAIVE Gaussian by using best params from Logistic regression

Use these params :

Best parameter set: {'clf__C': 1.0, 
                    'clf__penalty': 'l1', 
                     'vect__ngram_range': (1, 1), 
                     'vect__stop_words': None, 
                     'vect__tokenizer': <function tokenizer_porter at 0x7efcd931e6a8>}
"""

# THIS IS SAMPLE CODE FOR REFERENCE 

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.ensemble import RandomForestClassifier



##------------------Below is original code from class teaching

# Pipeline :  In layman language , this is nothing but the sequence of transformation the data will go before putting to final estimator (eg. clf MultinomialNB)

# GridSearch is nothing but it will iterate with iterator param_grid over Pipeline 

# TfidfVectorizer combines CountVectorizer and TfidTransformer with a single function.
tfidf = TfidfVectorizer(strip_accents=None, # Already preprocessed
                        lowercase=False,
                        preprocessor=None)

param_grid = {'vect__ngram_range': [(1, 1)],
              'vect__stop_words': [stopsPorter, None], # Not this time, but use idf with normalization
              'vect__tokenizer': [tokenizer_porter],
              'clf__n_estimators': [4], 
              'clf__max_features': ['log2'],
              'clf__criterion': ['entropy'], 
              'clf__max_depth': [2],
              'clf__min_samples_split': [2],
              'clf__min_samples_leaf': [1]}

NB_tfidf = Pipeline([('vect', tfidf),
                     ('clf', RandomForestClassifier())])
# Solver specified to silence warning and to enable l1 regularization

gs_NB_tfidf = GridSearchCV(estimator=NB_tfidf, param_grid=param_grid,
                           scoring='accuracy',
                           cv=5,
                           verbose=2,
                           n_jobs=1) # Number of jobs different from 1 sometimes crashes on Windows.

# The fitting of 2*2*2*3*5*2 models took around 30-60 minutes to fit in 2018. In 2019 it takes several hours. :(.
# Lowering the number of samples or parameters will make it quicker, but may reduce the performance greatly.
gs_NB_tfidf.fit(X_train, y_train)

# Pickle (store to disk) the Grid Search CV object
import pickle
with open('gs_NB_tfidf.pickle', 'wb') as f:
    pickle.dump(gs_NB_tfidf, f, pickle.HIGHEST_PROTOCOL)

# To open an object that has been pickled, you need to import the object's dependencies and local functions
import pickle
with open('gs_NB_tfidf.pickle', 'rb') as f:
    gs_lr_tfidf = pickle.load(f)

print('Best parameter set: %s ' % gs_NB_tfidf.best_params_)
print('CV Accuracy: %.3f' % gs_NB_tfidf.best_score_)

clf = gs_NB_tfidf.best_estimator_
print('Test Accuracy: %.3f' % clf.score(X_test, y_test))

#from sklearn.metrics import accuracy_score

#score = accuracy_score(y_test, preds)
#print (score)

"""#Training of NAIVE Without using BEST params from logistic regression"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

#Train and evaluate the model
vect = TfidfVectorizer().fit(X_train)
X_train_vectorized = vect.transform(X_train)
clfrNB = MultinomialNB(alpha = 0.1)
clfrNB.fit(X_train_vectorized, y_train)
preds = clfrNB.predict(vect.transform(X_test))

from sklearn.metrics import accuracy_score

score = accuracy_score(y_test, preds)
print (score)

# Pickle (store to disk) the Grid Search CV object
import pickle
with open('gs_lr_tfidf.pickle', 'wb') as f:
    pickle.dump(gs_lr_tfidf, f, pickle.HIGHEST_PROTOCOL)

"""# Use LDA for Unsupervised learning
Latent Dirichlet Allocation with scikit-learn
"""

import pandas as pd

df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/movie_data.csv', encoding='utf-8')
df.head(3)

#Words that occur across too many documents are exluded
from sklearn.feature_extraction.text import CountVectorizer

count = CountVectorizer(stop_words='english',
                        max_df=.1,         # Words that occur across too many documents are exluded
                        max_features=5000) # Most frequent words, limiting the dimensionality
                                           # Both can be tuned
X = count.fit_transform(df['review'].values)

# This may take 5+ minutes to compute

from sklearn.decomposition import LatentDirichletAllocation

lda = LatentDirichletAllocation(n_components=10,
                                random_state=123,
                                learning_method='batch')

# 'batch' uses all data in one go (most accurate), but slower than 'online' (online/mini-batch)

X_topics = lda.fit_transform(X)
lda.components_.shape

#Results of LDA
#Print the 5 most important words for each of the 10 topics

n_top_words = 5
feature_names = count.get_feature_names()

for topic_idx, topic in enumerate(lda.components_):
    print("Topic %d:" % (topic_idx + 1))
    print(" ".join([feature_names[i]
                    for i in topic.argsort()\
                        [:-n_top_words - 1:-1]]))

horror = X_topics[:, 5].argsort()[::-1]

for iter_idx, movie_idx in enumerate(horror[:3]):
    print('\nHorror movie #%d:' % (iter_idx + 1))
    print(df['review'][movie_idx][:300], '...')